{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Python Libraries"
      ],
      "metadata": {
        "id": "W7AqEHJoimG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfqx6Ubkh4HZ"
      },
      "outputs": [],
      "source": [
        "!pip install librosa matplotlib pandas \"tensorflow==2.8.*\" \"tensorflow-io==0.24.*\" \"tensorflow-model-optimization==0.7.2\"\n",
        "\n",
        "!pip install git+https://github.com/ARM-software/CMSIS_5.git@5.8.0#egg=CMSISDSP\\&subdirectory=CMSIS/DSP/PythonWrapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.utils.get_file('cmake-3.21.0-linux-x86_64.tar.gz',\n",
        "                        'https://github.com/Kitware/CMake/releases/download/v3.21.0/cmake-3.21.0-linux-x86_64.tar.gz',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='tools',\n",
        "                        extract=True)\n",
        "\n",
        "tf.keras.utils.get_file('gcc-arm-none-eabi-10-2020-q4-major-x86_64-linux.tar.bz2',\n",
        "                        'https://developer.arm.com/-/media/Files/downloads/gnu-rm/10-2020q4/gcc-arm-none-eabi-10-2020-q4-major-x86_64-linux.tar.bz2',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='tools',\n",
        "                        extract=True)"
      ],
      "metadata": {
        "id": "BNWijeskixs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xxd"
      ],
      "metadata": {
        "id": "UeEgJ7nMi1Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['PATH'] = f\"{os.getcwd()}/tools/cmake-3.21.0-linux-x86_64/bin:{os.environ['PATH']}\"\n",
        "os.environ['PATH'] = f\"{os.getcwd()}/tools/gcc-arm-none-eabi-10-2020-q4-major/bin:{os.environ['PATH']}\""
      ],
      "metadata": {
        "id": "jpTiwxpEi4FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raspberry Pi"
      ],
      "metadata": {
        "id": "EiH7I0vOi_-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "git clone --branch 1.2.0 https://github.com/raspberrypi/pico-sdk.git\n",
        "cd pico-sdk\n",
        "git submodule init\n",
        "git submodule update"
      ],
      "metadata": {
        "id": "wKUsYrV2jCHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PICO_SDK_PATH'] = f\"{os.getcwd()}/pico-sdk\""
      ],
      "metadata": {
        "id": "J4ph7hIIjIMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for SparkFun MicroMod\n",
        "# os.environ['PICO_BOARD'] = 'sparkfun_micromod'\n",
        "\n",
        "# for Raspberry Pi Pico (uncomment next line)\n",
        "os.environ['PICO_BOARD'] = 'pico'\n",
        "\n",
        "print(f\"PICO_BOARD env. var. set to '{os.environ['PICO_BOARD']}'\")"
      ],
      "metadata": {
        "id": "45aBmUl2jNnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "git clone --recurse-submodules https://github.com/ArmDeveloperEcosystem/ml-audio-classifier-example-for-pico.git"
      ],
      "metadata": {
        "id": "MmXBTAorjd9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ln -s ml-audio-classifier-example-for-pico/colab_utils colab_utils\n",
        "ln -s ml-audio-classifier-example-for-pico/inference-app inference-app"
      ],
      "metadata": {
        "id": "MOsZJw_Bjgun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.utils.get_file('esc-50.zip',\n",
        "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ],
      "metadata": {
        "id": "6Qh_gsnYjmKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "C6FsaUTxjpqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "df = pd.read_csv(esc50_csv)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "A-1P7Da2juCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "\n",
        "base_data_path = './datasets/ESC-50-master/audio/'\n",
        "\n",
        "df['fullpath'] = df['filename'].map(lambda x: path.join(base_data_path, x))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dhyPCzYGjx2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_io as tfio\n",
        "import librosa\n",
        "\n",
        "def load_wav(filename, desired_sample_rate, desired_channels):\n",
        "  try:\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=desired_channels)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "  except:\n",
        "    # fallback to librosa if the wav file can be read with TF\n",
        "    filename = tf.cast(filename, tf.string)\n",
        "    wav, sample_rate = librosa.load(filename.numpy().decode('utf-8'), sr=None, mono=(desired_channels == 1))\n",
        "\n",
        "  wav = tfio.audio.resample(wav, rate_in=tf.cast(sample_rate, dtype=tf.int64), rate_out=tf.cast(desired_sample_rate, dtype=tf.int64))\n",
        "\n",
        "  return wav"
      ],
      "metadata": {
        "id": "vwzTtCRHj0aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "sample_rate = 16000\n",
        "channels = 1\n",
        "\n",
        "test_wav_file_path = df['fullpath'][0]\n",
        "test_wav_data = load_wav(test_wav_file_path, sample_rate, channels)\n",
        "\n",
        "plt.plot(test_wav_data)\n",
        "plt.show()\n",
        "\n",
        "display.Audio(test_wav_data, rate=sample_rate)"
      ],
      "metadata": {
        "id": "CUwpdir8j3DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.plot(test_wav_data[32000:48000])"
      ],
      "metadata": {
        "id": "_tMMTK3jj47C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fullpaths = df['fullpath']\n",
        "targets = df['target']\n",
        "folds = df['fold']\n",
        "\n",
        "fullpaths_ds = tf.data.Dataset.from_tensor_slices((fullpaths, targets, folds))\n",
        "fullpaths_ds.element_spec"
      ],
      "metadata": {
        "id": "T_O2yPVpj7v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_wav_for_map(fullpath, label, fold):\n",
        "  wav = tf.py_function(load_wav, [fullpath, sample_rate, channels], tf.float32)\n",
        "\n",
        "  return wav, label, fold\n",
        "\n",
        "wav_ds = fullpaths_ds.map(load_wav_for_map)\n",
        "wav_ds.element_spec"
      ],
      "metadata": {
        "id": "UwlET57rj-IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def split_wav(wav, width, stride):\n",
        "  return tf.map_fn(fn=lambda t: wav[t * stride:t * stride + width], elems=tf.range((tf.shape(wav)[0] - width) // stride), fn_output_signature=tf.float32)\n",
        "\n",
        "@tf.function\n",
        "def wav_not_empty(wav):\n",
        "  return tf.experimental.numpy.any(wav)\n",
        "\n",
        "def split_wav_for_flat_map(wav, label, fold):\n",
        "  wavs = split_wav(wav, width=16000, stride=4000)\n",
        "  labels = tf.repeat(label, tf.shape(wavs)[0])\n",
        "  folds = tf.repeat(fold, tf.shape(wavs)[0])\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices((wavs, labels, folds))\n",
        "\n",
        "split_wav_ds = wav_ds.flat_map(split_wav_for_flat_map)\n",
        "split_wav_ds = split_wav_ds.filter(lambda x, y, z: wav_not_empty(x))"
      ],
      "metadata": {
        "id": "AEc5lLRTkAsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for wav, _, _ in split_wav_ds.take(5):\n",
        "  _ = plt.plot(wav)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pH7UByqCkDIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Spectrograms"
      ],
      "metadata": {
        "id": "7NT2kDGGkGTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def create_spectrogram(samples):\n",
        "  return tf.abs(\n",
        "      tf.signal.stft(samples, frame_length=256, frame_step=128)\n",
        "  )"
      ],
      "metadata": {
        "id": "kGGSF4NckEpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spectrogram = create_spectrogram(test_wav_data[32000:48000])\n",
        "\n",
        "spectrogram.shape"
      ],
      "metadata": {
        "id": "5WYRj2h9kLmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_spectrogram(spectrogram, vmax=None):\n",
        "  transposed_spectrogram = tf.transpose(spectrogram)\n",
        "\n",
        "  fig = plt.figure(figsize=(8,6))\n",
        "  height = transposed_spectrogram.shape[0]\n",
        "  X = np.arange(transposed_spectrogram.shape[1])\n",
        "  Y = np.arange(height * int(sample_rate / 256), step=int(sample_rate / 256))\n",
        "\n",
        "  im = plt.pcolormesh(X, Y, tf.transpose(spectrogram), vmax=vmax)\n",
        "\n",
        "  fig.colorbar(im)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_spectrogram(spectrogram)"
      ],
      "metadata": {
        "id": "lAwEi4iDkOEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spectrogram_for_map(samples, label, fold):\n",
        "  return create_spectrogram(samples), label, fold\n",
        "\n",
        "spectrograms_ds = split_wav_ds.map(create_spectrogram_for_map)\n",
        "spectrograms_ds.element_spec"
      ],
      "metadata": {
        "id": "kTFnci3XkPR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s, _, _ in spectrograms_ds.take(5):\n",
        "  plot_spectrogram(s)"
      ],
      "metadata": {
        "id": "MfJw3fbvkRHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Dataset"
      ],
      "metadata": {
        "id": "zHetIPREkWeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "random_seed = 42\n",
        "tf.random.set_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ],
      "metadata": {
        "id": "6kq8s-ZKkYOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cached_ds = spectrograms_ds.cache()\n",
        "\n",
        "train_ds = cached_ds.filter(lambda spectrogram, label, fold: fold < 4)\n",
        "val_ds = cached_ds.filter(lambda spectrogram, label, fold: fold == 4)\n",
        "test_ds = cached_ds.filter(lambda spectrogram, label, fold: fold > 4)\n",
        "\n",
        "# remove the folds column as it's no longer needed\n",
        "remove_fold_column = lambda spectrogram, label, fold: (tf.expand_dims(spectrogram, axis=-1), label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000, seed=random_seed).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "N7abc684kaqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "vWZwf5GjkesE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for spectrogram, _, _ in cached_ds.take(1):\n",
        "    input_shape = tf.expand_dims(spectrogram, axis=-1).shape\n",
        "    print('Input shape:', input_shape)\n",
        "\n",
        "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "norm_layer.adapt(cached_ds.map(lambda x, y, z: tf.reshape(x, input_shape)))"
      ],
      "metadata": {
        "id": "yRv0Fi4Hkd5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=input_shape),\n",
        "  tf.keras.layers.experimental.preprocessing.Resizing(32, 32, interpolation=\"nearest\"),\n",
        "  norm_layer,\n",
        "  tf.keras.layers.Conv2D(8, kernel_size=(8,8), strides=(2, 2), activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dropout(0.25),\n",
        "  tf.keras.layers.Dense(50, activation='softmax')\n",
        "])\n",
        "\n",
        "baseline_model.summary()"
      ],
      "metadata": {
        "id": "MYBcUwIAkjnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      \"accuracy\",\n",
        "]\n",
        "\n",
        "baseline_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=METRICS,\n",
        ")\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 100:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(verbose=1, patience=25),\n",
        "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "]"
      ],
      "metadata": {
        "id": "MbcI7Kf0kln-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 250\n",
        "history = baseline_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "id": "51w2P_QxkokV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "yJ16e4VGkrJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model.save(\"baseline_model\")"
      ],
      "metadata": {
        "id": "8JVU2qB9ku9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r baseline_model.zip baseline_model"
      ],
      "metadata": {
        "id": "dh1Bx00ckwkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download datasets"
      ],
      "metadata": {
        "id": "zc3KUwhGk5dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.get_file('dog_barks.tar.gz',\n",
        "                        'https://github.com/seaweed2018/180DA-WarmUp/raw/main/ml-audio-classifier-example-for-pico-dog_barks.tar.gz',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)"
      ],
      "metadata": {
        "id": "AMJtEtKzk43p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we only need the files in the _background_noise_ folder of the dataset\n",
        "# use the curl command to download the archive file and then manually extract\n",
        "# using the tar command, instead of using tf.keras.utils.get_file(...)\n",
        "# in Python\n",
        "\n",
        "!mkdir -p datasets/speech_commands\n",
        "!curl -L -o datasets/speech_commands_v0.02.tar.gz http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "!tar --wildcards --directory datasets/speech_commands -xzvf datasets/speech_commands_v0.02.tar.gz './_background_noise_/*'"
      ],
      "metadata": {
        "id": "yNI_r-40lAH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls datasets"
      ],
      "metadata": {
        "id": "VqVeTA8tlBrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_bark_files_ds = tf.data.Dataset.list_files(\"datasets/ml-audio-classifier-example-for-pico-dog_barks/*.wav\", shuffle=False)\n",
        "dog_bark_files_ds = dog_bark_files_ds.map(lambda x: (x, 1, -1))"
      ],
      "metadata": {
        "id": "wL8DdUXYlFLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "background_noise_files_ds = tf.data.Dataset.list_files(\"datasets/speech_commands/_background_noise_/*.wav\", shuffle=False)\n",
        "background_noise_files_ds = background_noise_files_ds.map(lambda x: (x, 0, -1))"
      ],
      "metadata": {
        "id": "7ZtBZNWTlGfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_bark_wav_ds = dog_bark_files_ds.map(load_wav_for_map)\n",
        "dog_bark_wav_ds = dog_bark_wav_ds.cache()\n",
        "\n",
        "background_noise_wav_ds = background_noise_files_ds.map(load_wav_for_map)\n",
        "background_noise_wav_ds = background_noise_wav_ds.cache()"
      ],
      "metadata": {
        "id": "mVGsmhXnlH_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for wav_data, _, _ in dog_bark_wav_ds.take(1):\n",
        "  plt.plot(wav_data)\n",
        "  plt.ylim([-1, 1])\n",
        "  plt.show()\n",
        "\n",
        "  display.display(display.Audio(wav_data, rate=sample_rate))"
      ],
      "metadata": {
        "id": "wNCnOcLPlJZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for wav_data, _, _ in background_noise_wav_ds.take(1):\n",
        "  plt.plot(wav_data)\n",
        "  plt.ylim([-1, 1])\n",
        "  plt.show()\n",
        "\n",
        "  display.display(display.Audio(wav_data, rate=sample_rate))"
      ],
      "metadata": {
        "id": "el6AOO-_lMKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dog_bark_wav_ds = dog_bark_wav_ds.flat_map(split_wav_for_flat_map)\n",
        "split_dog_bark_wav_ds = split_dog_bark_wav_ds.filter(lambda x, y, z: wav_not_empty(x))\n",
        "\n",
        "split_background_noise_wav_ds = background_noise_wav_ds.flat_map(split_wav_for_flat_map)\n",
        "split_background_noise_wav_ds = split_background_noise_wav_ds.filter(lambda x, y, z: wav_not_empty(x))"
      ],
      "metadata": {
        "id": "odalO_iklPmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow"
      ],
      "metadata": {
        "id": "OHpkiv8QlSV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cmsisdsp\n",
        "from numpy import pi as PI\n",
        "\n",
        "window_size = 256\n",
        "step_size = 128\n",
        "\n",
        "hanning_window_f32 = np.zeros(window_size)\n",
        "for i in range(window_size):\n",
        "  hanning_window_f32[i] = 0.5 * (1 - cmsisdsp.arm_cos_f32(2 * PI * i / window_size ))\n",
        "\n",
        "hanning_window_q15 = cmsisdsp.arm_float_to_q15(hanning_window_f32)\n",
        "\n",
        "rfftq15 = cmsisdsp.arm_rfft_instance_q15()\n",
        "status = cmsisdsp.arm_rfft_init_q15(rfftq15, window_size, 0, 1)\n",
        "\n",
        "def get_arm_spectrogram(waveform):\n",
        "\n",
        "  num_frames = int(1 + (len(waveform) - window_size) // step_size)\n",
        "  fft_size = int(window_size // 2 + 1)\n",
        "\n",
        "  # Convert the audio to q15\n",
        "  waveform_q15 = cmsisdsp.arm_float_to_q15(waveform)\n",
        "\n",
        "  # Create empty spectrogram array\n",
        "  spectrogram_q15 = np.empty((num_frames, fft_size), dtype = np.int16)\n",
        "\n",
        "  start_index = 0\n",
        "\n",
        "  for index in range(num_frames):\n",
        "    # Take the window from the waveform.\n",
        "    window = waveform_q15[start_index:start_index + window_size]\n",
        "\n",
        "    # Apply the Hanning Window.\n",
        "    window = cmsisdsp.arm_mult_q15(window, hanning_window_q15)\n",
        "\n",
        "    # Calculate the FFT, shift by 7 according to docs\n",
        "    window = cmsisdsp.arm_rfft_q15(rfftq15, window)\n",
        "\n",
        "    # Take the absolute value of the FFT and add to the Spectrogram.\n",
        "    spectrogram_q15[index] = cmsisdsp.arm_cmplx_mag_q15(window)[:fft_size]\n",
        "\n",
        "    # Increase the start index of the window by the overlap amount.\n",
        "    start_index += step_size\n",
        "\n",
        "  # Convert to numpy output ready for keras\n",
        "  return cmsisdsp.arm_q15_to_float(spectrogram_q15).reshape(num_frames,fft_size) * 512"
      ],
      "metadata": {
        "id": "HzK82GTQlYE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def create_arm_spectrogram_for_map(wav, label, fold):\n",
        "  spectrogram = tf.py_function(get_arm_spectrogram, [wav], tf.float32)\n",
        "\n",
        "  return spectrogram, label, fold\n",
        "\n",
        "dog_bark_spectrograms_ds = split_dog_bark_wav_ds.map(create_arm_spectrogram_for_map)\n",
        "dog_bark_spectrograms_ds = dog_bark_spectrograms_ds.cache()\n",
        "\n",
        "for spectrogram, _, _ in dog_bark_spectrograms_ds.take(1):\n",
        "  plot_spectrogram(spectrogram)"
      ],
      "metadata": {
        "id": "hTkSqHR3lcqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "background_noise_spectrograms_ds = split_background_noise_wav_ds.map(create_arm_spectrogram_for_map)\n",
        "background_noise_spectrograms_ds = background_noise_spectrograms_ds.cache()\n",
        "\n",
        "for spectrogram, _, _ in background_noise_spectrograms_ds.take(1):\n",
        "  plot_spectrogram(spectrogram)"
      ],
      "metadata": {
        "id": "-THYhhgDlf0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ds_len(ds):\n",
        "  count = 0\n",
        "  for _, _, _ in ds:\n",
        "    count += 1\n",
        "\n",
        "  return count\n",
        "\n",
        "num_dog_bark_spectrograms = calculate_ds_len(dog_bark_spectrograms_ds)\n",
        "num_background_noise_spectrograms = calculate_ds_len(background_noise_spectrograms_ds)\n",
        "\n",
        "print(f\"num_dog_bark_spectrograms = {num_dog_bark_spectrograms}\")\n",
        "print(f\"num_background_noise_spectrograms = {num_background_noise_spectrograms}\")"
      ],
      "metadata": {
        "id": "bcmQw3ARliDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "MWDUSwellk6Z"
      }
    }
  ]
}